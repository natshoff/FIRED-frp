{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0ffac3",
   "metadata": {},
   "source": [
    "# **FRP Code Workflow Notebook**\n",
    "\n",
    "### **Purpose**\n",
    "Calculate cumulatve fire radiative power (cumFRP in W/Km^2) from an active fire detection source (AFD), for a specific area of interest (AOI), using a fire perimeter data set to bound detection.\n",
    "\n",
    "### **Inputs**\n",
    "**Active Fire Detection:** VIIRS\n",
    "\n",
    "**Area of interest (AOI):** Southern Rockies Ecoregion\n",
    "\n",
    "**Fire Perimeters:** FIRED (post-processed 5/11)\n",
    "\n",
    "### **Outputs**\n",
    "[WIP]\n",
    "\n",
    "### **Author Acknowledgement**\n",
    "Maxwell Cook (maxwell.cook@colorado.edu) created the original workflow and methods for aggregating FRP\n",
    "\n",
    "Nate Hofford (nate.hofford@colorado.edu) modified this original workflow to be compatible with various AFDs, AOIs, and fire perimeters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350d226",
   "metadata": {},
   "source": [
    "## **Step 0: Set working directory and import python libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecc696-cd36-4140-b719-df8bcf5c6a9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import sys, os, math\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import fiona # specific for reading in .gpd files (like Welty and Jeffries)\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rasterio as rio\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from rasterio.features import rasterize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "## Projection information\n",
    "geog = 'EPSG:4326'  # Geographic projection\n",
    "prj = 'EPSG:5070'  # Projected coordinate system- WGS 84 NAD83 UTM Zone 13N\n",
    "\n",
    "\n",
    "## Working directories\n",
    "# File path information\n",
    "_current_cwd = Path.cwd().resolve() # Get current working directory at the start of this cell\n",
    "_projdir_path = None\n",
    "\n",
    "# Running from code dir\n",
    "# This is common if the notebook is in 'code/' and Jupyter's CWD is the notebook's directory.\n",
    "if _current_cwd.name == 'code':\n",
    "    _parent_dir = _current_cwd.parent\n",
    "    # Verify that this parent directory indeed contains the 'code' directory (which is _current_cwd)\n",
    "    if (_parent_dir / 'code').exists() and (_parent_dir / 'code').resolve().samefile(_current_cwd.resolve()):\n",
    "        _projdir_path = _parent_dir\n",
    "\n",
    "# Running from root dir\n",
    "# This is common if Jupyter is started from the project root.\n",
    "elif (_current_cwd / 'code').is_dir():\n",
    "    _projdir_path = _current_cwd\n",
    "\n",
    "# Error handling\n",
    "if _projdir_path is None:\n",
    "    print(f\"WARNING: Could not automatically determine project root using standard heuristics from CWD: {_current_cwd}.\")\n",
    "    print(f\"Defaulting projdir: If CWD is 'code', its parent is used. Otherwise, CWD itself ('{_current_cwd}') is used.\")\n",
    "    if _current_cwd.name == 'code':\n",
    "        _projdir_path = _current_cwd.parent\n",
    "    else:\n",
    "        _projdir_path = _current_cwd # Assume CWD is project root as a last resort\n",
    "\n",
    "projdir = str(_projdir_path.resolve()) # Ensure projdir is an absolute string path\n",
    "\n",
    "# Add the 'code' directory (expected to be projdir/code/) to sys.path\n",
    "_custom_functions_path = os.path.join(projdir, 'code')\n",
    "if _custom_functions_path not in sys.path:\n",
    "    sys.path.insert(0, _custom_functions_path)\n",
    "\n",
    "# Set Current Working Directory to Project Directory\n",
    "if Path.cwd().resolve() != Path(projdir).resolve():\n",
    "    os.chdir(projdir)\n",
    "    print(f\"Project directory set to: {projdir}\")\n",
    "    print(f\"Changed working directory to project root: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Project directory set to: {projdir}\")\n",
    "    print(f\"Working directory is already project root: {os.getcwd()}\")\n",
    "\n",
    "# Output directories\n",
    "dataAFD = os.path.join(projdir,'data/output/AFD')\n",
    "dataFires = os.path.join(projdir,'data/output/firePerimeters')\n",
    "\n",
    "# Custom functions\n",
    "from __functions import *\n",
    "\n",
    "\n",
    "## You done and this is good to go\n",
    "print(\"Ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82d6892-66d3-4e5d-ad61-c35e5a972f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to re-run if there were any edits to the __functions.py file\n",
    "import importlib\n",
    "import __functions\n",
    "importlib.reload(__functions)\n",
    "from __functions import *  # re-import everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d8d2b",
   "metadata": {},
   "source": [
    "## **Step 1: Choose an area of interest (AOI)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924ee77-116c-42ec-a734-20a122446c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select area of interest \n",
    "fp = os.path.join(projdir,'data/input/AOI/westUS_5070.gpkg')\n",
    "aoi = gpd.read_file(fp)\n",
    "# calculate the total bounds with a 10km buffer\n",
    "bounds = aoi.geometry.unary_union.envelope.buffer(10000) # buffer in m\n",
    "aoi.geometry.unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1324664",
   "metadata": {},
   "source": [
    "## **Step 2: Choose a fire perimeter dataset**\n",
    "\n",
    "In this example, we are using **FIRED v2 (5/11)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160f724-1e0e-4174-b373-0d55fce64044",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Used FIRED CONUS perimeters\n",
    "# Load and tidy the fire perimeter data\n",
    "fp = os.path.join(projdir,'data/input/firePerimeters/FIRED/fired_conus_ak_2000_to_2025_events_merged.gpkg')\n",
    "fires = gpd.read_file(fp)\n",
    "\n",
    "print(fires.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37773e2c",
   "metadata": {},
   "source": [
    "## **Step 2a (cont): Extract relevant columns and dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8f2932-339b-4586-a109-5692e46704e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIRED Specific Values\n",
    "\n",
    "## Needed to change the columns here for FIRED specific values\n",
    "# tidy the date columns\n",
    "## DISCOVERY_DATE\n",
    "fires['ig_date'] = pd.to_datetime(fires['ig_date'])\n",
    "## WF_CESSATION_DATE\n",
    "fires['last_date'] = pd.to_datetime(fires['last_date'])\n",
    "## Final_Acres = now will be km2\n",
    "fires['tot_ar_km2'] = fires['tot_ar_km2'].astype(float)\n",
    "## ICS_ACRES\n",
    "#fires['ICS_ACRES'] = fires['ICS_ACRES'].astype(float)\n",
    "\n",
    "# subset the columns\n",
    "#fires = fires[['Fire_ID','Fire_Name','Final_Acres',\n",
    "#               'INCIDENT_ID','START_YEAR','DISCOVERY_DATE',\n",
    "#               'WF_CESSATION_DATE','ICS_ACRES','geometry']]\n",
    "\n",
    "# subset the columns\n",
    "fires = fires[['merge_id','tot_ar_km2','ig_year','ig_date',\n",
    "               'last_date','geometry']]\n",
    "\n",
    "# remove any duplicates (the RYAN fire was duplicated for some reason)\n",
    "fires = fires.drop_duplicates(subset='merge_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97376d",
   "metadata": {},
   "source": [
    "## **Step 3a: Filter fire perimeters for AOI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79f3ea-1f5f-43c1-8a37-50f9859d308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer CRS\n",
    "print(f\"CRS of 'fires': {fires.crs}\")\n",
    "print(f\"CRS of 'aoi': {aoi.crs}\")\n",
    "\n",
    "# Ensure the CRS matches between fires and aoi\n",
    "if fires.crs != aoi.crs:\n",
    "    aoi = aoi.to_crs(fires.crs)\n",
    "\n",
    "# Create a single unified geometry for the aoi region\n",
    "aoi_union = aoi.geometry.unary_union\n",
    "\n",
    "# Filter fires to those that intersect the Southern Rockies\n",
    "fires_aoi = fires[fires.geometry.intersects(aoi_union)].copy()\n",
    "\n",
    "# Export\n",
    "fires_aoi.to_file(os.path.join(dataFires,\"firesWEST_FIRED.gpkg\"))\n",
    "\n",
    "# Print\n",
    "print(f\"Filtered fires to {len(fires_aoi)} records that intersect the area of interest.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851961f1",
   "metadata": {},
   "source": [
    "## **Step 3b: Buffer fire perimeters + take the convex hull**\n",
    "Here we buffer all FIRED detections by 3 km and then take the convex hull to ensure we collect all AFD within the fire perimeter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "buffer_dist = 3000  # Buffer distance in meters\n",
    "# Target CRS for the output. The original code block used EPSG:5070.\n",
    "# If this fires_aoi is primarily used next with AFD data in EPSG:4326,\n",
    "# you might consider changing target_crs to 'EPSG:4326' here.\n",
    "target_crs_for_output = 'EPSG:5070'\n",
    "output_filename = \"fires_FIRED_buffered_hull.gpkg\"\n",
    "\n",
    "# It's assumed 'fires_aoi' is an existing GeoDataFrame with fire perimeters and metadata.\n",
    "# It's also assumed 'dataFires' (output directory path) and 'os' are defined.\n",
    "# Also, 'gpd' (geopandas) should be imported.\n",
    "\n",
    "print(f\"Processing {len(fires_aoi)} fire perimeters.\")\n",
    "print(f\"Original CRS of 'fires_aoi': {fires_aoi.crs}\")\n",
    "\n",
    "# --- Step 1: Buffer each fire perimeter ---\n",
    "# This operation is applied row-wise, and GeoPandas preserves\n",
    "# all existing columns (metadata) for each fire.\n",
    "print(f\"Buffering perimeters by {buffer_dist/1000} km...\")\n",
    "fires_aoi['geometry'] = fires_aoi.geometry.buffer(buffer_dist)\n",
    "print(\"Buffering complete.\")\n",
    "\n",
    "# --- Step 2: Calculate the convex hull for each buffered fire perimeter ---\n",
    "# This operation is also applied row-wise to the 'geometry' column.\n",
    "# All other columns (metadata, including 'merge_id', 'tot_ar_km2', 'ig_date', etc.)\n",
    "# are preserved for each feature.\n",
    "print(\"Calculating convex hull for each buffered perimeter...\")\n",
    "fires_aoi['geometry'] = fires_aoi.geometry.convex_hull\n",
    "print(\"Convex hull calculation complete.\")\n",
    "\n",
    "# --- Step 3: Ensure 'fires_aoi' is in the desired target CRS for output ---\n",
    "current_crs = fires_aoi.crs\n",
    "if str(current_crs).upper() != target_crs_for_output.upper():\n",
    "    print(f\"Reprojecting 'fires_aoi' (now containing hulls) from {current_crs} to {target_crs_for_output}...\")\n",
    "    fires_aoi = fires_aoi.to_crs(target_crs_for_output)\n",
    "    print(f\"Reprojection complete. 'fires_aoi' is now in {target_crs_for_output}.\")\n",
    "else:\n",
    "    print(f\"'fires_aoi' (hulls) is already in {target_crs_for_output}.\")\n",
    "\n",
    "# --- Step 4: Export the GeoDataFrame ---\n",
    "# 'fires_aoi' now contains:\n",
    "#   - Original 'merge_id' (and all other metadata columns)\n",
    "#   - 'geometry' column with the convex hull of the buffered perimeter\n",
    "#   - Correctly projected to 'target_crs_for_output'\n",
    "output_full_path = os.path.join(dataFires, output_filename)\n",
    "fires_aoi.to_file(output_full_path, driver=\"GPKG\")\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(fires_aoi)} fire perimeters.\")\n",
    "print(f\"Final geometries are convex hulls of {buffer_dist/1000} km buffered perimeters.\")\n",
    "print(f\"All original metadata preserved for each hull.\")\n",
    "print(f\"Exported to: {output_full_path} (CRS: {fires_aoi.crs})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805447c1",
   "metadata": {},
   "source": [
    "## **Step 4: Choose active fire detection (AFD) dataset**\n",
    "\n",
    "In this example, we are using **VIIRS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88442dc-d433-4ef3-9d09-62f082e96755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display csv to see data structure\n",
    "df = pd.read_csv(\"data/input/AFD/VIIRS/fire_nrt_J1V-C2_603779.csv\")\n",
    "\n",
    "# Show the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c97d5",
   "metadata": {},
   "source": [
    "## **Step 4a: Data cleaning**\n",
    "\n",
    "- Remove duplicate observations within each satellite across archived (higher quality) and NRT (near real time, so less processing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf2605-abea-42cf-afcf-cd8d62ce7cf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File paths for the NRT and Archive data files\n",
    "nrt_files = {\n",
    "    'J1V-C2': 'data/input/AFD/VIIRS/fire_nrt_J1V-C2_603779.csv',\n",
    "    'J2V-C2': 'data/input/AFD/VIIRS/fire_nrt_J2V-C2_603780.csv',\n",
    "    'SV-C2': 'data/input/AFD/VIIRS/fire_nrt_SV-C2_603781.csv'\n",
    "    }\n",
    "\n",
    "archive_files = {\n",
    "    'J1V-C2': 'data/input/AFD/VIIRS/fire_archive_J1V-C2_603779.csv',\n",
    "    'SV-C2': 'data/input/AFD/VIIRS/fire_archive_SV-C2_603781.csv'\n",
    "    }\n",
    "\n",
    "# Function to process and deduplicate data files within each satellite\n",
    "def process_and_deduplicate(nrt_file, archive_file, satellite_type):\n",
    "    # Read the NRT and Archive data files\n",
    "    nrt_data = pd.read_csv(nrt_file)\n",
    "    archive_data = pd.read_csv(archive_file)\n",
    "\n",
    "    # Add source and satellite type columns\n",
    "    nrt_data['source'] = 'NRT'\n",
    "    archive_data['source'] = 'Archive'\n",
    "    nrt_data['satellite_type'] = satellite_type\n",
    "    archive_data['satellite_type'] = satellite_type\n",
    "\n",
    "    # Concatenate NRT and Archive data\n",
    "    combined_data = pd.concat([nrt_data, archive_data])\n",
    "\n",
    "    # Format acquisition time to HHMM and convert date + time to datetime\n",
    "    combined_data['acq_time'] = combined_data['acq_time'].astype(str).str.zfill(4)\n",
    "    combined_data['acq_datetime'] = pd.to_datetime(\n",
    "        combined_data['acq_date'].astype(str) + ' ' +\n",
    "        combined_data['acq_time'].str[:2] + ':' +\n",
    "        combined_data['acq_time'].str[2:] + ':00',\n",
    "        format='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Count duplicates before removing them\n",
    "    duplicates_mask = combined_data.duplicated(subset=['latitude', 'longitude', 'acq_datetime'], keep=False)\n",
    "    duplicates_count = duplicates_mask.sum()\n",
    "    print(f\"Number of duplicates in {satellite_type}: {duplicates_count}\")\n",
    "\n",
    "    # Deduplicate, prioritizing Archive data\n",
    "    deduplicated_data = combined_data.sort_values(by=['acq_datetime', 'source'], ascending=[False, True])\n",
    "    deduplicated_data = deduplicated_data.drop_duplicates(subset=['latitude', 'longitude', 'acq_datetime'], keep='first')\n",
    "\n",
    "    return deduplicated_data\n",
    "\n",
    "# Define the path for the final output file\n",
    "final_csv_path = 'data/input/AFD/VIIRS/VIIRS_cat.csv'\n",
    "\n",
    "# Check if the final deduplicated file already exists\n",
    "if os.path.exists(final_csv_path):\n",
    "    print(\"Deduplicated and concatenated data already exists!\")\n",
    "else:\n",
    "    # Process each satellite type\n",
    "    deduplicated_datasets = []\n",
    "\n",
    "    for satellite_type, nrt_file in nrt_files.items():\n",
    "        archive_file = archive_files.get(satellite_type)\n",
    "        if archive_file:\n",
    "            deduplicated_data = process_and_deduplicate(nrt_file, archive_file, satellite_type)\n",
    "            deduplicated_datasets.append(deduplicated_data)\n",
    "\n",
    "    # Combine all deduplicated datasets\n",
    "    if deduplicated_datasets: # Check if there's any data to concatenate\n",
    "        VIIRS_cat = pd.concat(deduplicated_datasets)\n",
    "\n",
    "        # Save the final deduplicated dataset\n",
    "        VIIRS_cat.to_csv(final_csv_path, index=False)\n",
    "        print(\"All satellite data processed and deduplicated successfully!\")\n",
    "    else:\n",
    "        print(\"No data to process or combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9789df-2f68-4e51-9d89-53cef724d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the AFD data from the VIIRS censors\n",
    "fp = os.path.join('data/input/AFD/VIIRS/VIIRS_cat.csv')\n",
    "afds = pd.read_csv(fp, dtype={'version': str}).reset_index(drop=True)\n",
    "afds = afds.loc[:, ~afds.columns.str.startswith('Unnamed:')]\n",
    "print(f\"Number of fire detections: {len(afds)}\")\n",
    "afds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de4225-ac37-468b-93ee-384ed4f8fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop low confidence detections\n",
    "\n",
    "## NH: potentially keep this step or keep all the fires and allow people to filter after by keeping the confidence column\n",
    "N = len(afds)\n",
    "print(afds['confidence'].value_counts())\n",
    "afds = afds[afds['confidence'] != 'l']\n",
    "print(f\"\\nDropped {N-len(afds)} [{round(((N-len(afds))/N)*100,2)}%] low-confidence obs.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae4702",
   "metadata": {},
   "source": [
    "## **Step 5: Join AFD and fire perimeters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74507701-7088-4739-91c8-39be8ca04834",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create spatial points from lat/lon\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# convert to spatial points using pixel centroid\n",
    "afds['geometry'] = [Point(xy) for xy in zip(afds.longitude, afds.latitude)]\n",
    "afds_ll = gpd.GeoDataFrame(afds, geometry='geometry', crs=\"EPSG:4326\")\n",
    "afds_ll = afds_ll.to_crs(\"EPSG:5070\")\n",
    "afds_ll = afds_ll.reset_index(drop=True)\n",
    "afds_ll['afdID'] = afds_ll.index # add a unique ID\n",
    "print(afds_ll[['longitude', 'latitude', 'confidence', 'frp', 'acq_date', 'acq_time', 'daynight', 'satellite']].head())\n",
    "\n",
    "# Ensure 'bounds' (AOI) is in EPSG:5070 to match afds_ll\n",
    "target_crs_str = 'EPSG:5070'\n",
    "if hasattr(bounds, 'crs'): # Check if bounds is a GeoDataFrame/GeoSeries\n",
    "    if bounds.crs != target_crs_str:\n",
    "        print(f\"Reprojecting 'bounds' from {bounds.crs} to {target_crs_str}...\")\n",
    "        bounds = bounds.to_crs(target_crs_str)\n",
    "        print(\"'bounds' reprojection complete.\")\n",
    "    else:\n",
    "        print(f\"'bounds' is already in {target_crs_str}.\")\n",
    "else:\n",
    "    # If bounds is a Shapely geometry, it has no CRS attribute.\n",
    "    # It's assumed to be in the same CRS as afds_ll (EPSG:5070) for the .within() operation.\n",
    "    print(f\"'bounds' is a Shapely geometry. Assuming it is in {target_crs_str} for the spatial subset.\")\n",
    "    print(\"Ensure 'bounds' was derived from data in, or transformed to, EPSG:5070.\")\n",
    "\n",
    "# spatially subset the AOI extent\n",
    "afds_ll = afds_ll[afds_ll.geometry.within(bounds)]\n",
    "print(f\"\\n[{len(afds_ll)}({round(len(afds_ll)/len(afds)*100,2)}%)] detections in the area of interest.\")\n",
    "\n",
    "print(afds_ll.columns.tolist())\n",
    "\n",
    "\n",
    "# save this file out.\n",
    "## Do a spatial join with these points to see the % that overlaps with FIRED perims\n",
    "out_fp = os.path.join(dataAFD,'viirs_snpp_jpss1_afdFIRED_latlon_west.gpkg')\n",
    "afds_ll.to_file(out_fp)\n",
    "print(f\"\\nSaved spatial points to: {out_fp}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fa8db",
   "metadata": {},
   "source": [
    "## **Step 6: Join AFD and fire perimeters and remove AFD detection duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39b7ce-792b-456a-a0d1-2a4f197b8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure both fire perims and AFD observations are in the same CRS\n",
    "fires_aoi = fires_aoi.to_crs(afds_ll.crs)\n",
    "\n",
    "# spatially join to the fire perimeters\n",
    "# retain a subset of fire columns in the join\n",
    "afds_ll_fires = gpd.sjoin(\n",
    "    afds_ll, fires_aoi[['merge_id','tot_ar_km2','ig_year','ig_date',\n",
    "               'last_date','geometry']], \n",
    "    how='inner', \n",
    "    predicate='within'\n",
    ").drop(columns=['index_right']) # drop the join index column\n",
    "\n",
    "# check for duplicate rows (same AFD, multiple fires)\n",
    "## NH: removing the smallest duplicate appropriate or is there a better way to handle these VIIRS duplictes?\n",
    "duplicates = afds_ll_fires[afds_ll_fires.duplicated(subset='afdID', keep=False)]\n",
    "print(f\"\\nResolving [{len(duplicates)}/{len(afds_ll_fires)}] duplicate obs.\\n\")\n",
    "\n",
    "# create an acquisition datetime column\n",
    "# set the time in MST as well\n",
    "# see __functions.py\n",
    "afds_ll_fires['acq_dt_mst'] = afds_ll_fires.apply(\n",
    "    lambda row: convert_datetime(\n",
    "        row['acq_date'], \n",
    "        row['acq_time'], \n",
    "        zone='America/Denver' # MST\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# tidy & prep the temporal information\n",
    "afds_ll_fires['acq_date'] = pd.to_datetime(afds_ll_fires['acq_date'])\n",
    "afds_ll_fires['acq_month'] = afds_ll_fires['acq_date'].dt.month.astype(int) # extract the month\n",
    "afds_ll_fires['acq_year'] = afds_ll_fires['acq_date'].dt.year.astype(int) # extract the year\n",
    "# filter the AFDs by temporal matches to their fire event\n",
    "# identify observations within 14 days of ignition/cessation\n",
    "afds_ll_fires = afds_ll_fires[\n",
    "    (afds_ll_fires['acq_date'] >= afds_ll_fires['ig_date'] - timedelta(days=14)) &\n",
    "    (afds_ll_fires['acq_date'] <= afds_ll_fires['last_date'] + timedelta(days=14))\n",
    "]\n",
    "\n",
    "print(afds_ll_fires.columns.tolist())\n",
    "\n",
    "# re-check duplicates\n",
    "duplicates = afds_ll_fires[afds_ll_fires.duplicated(subset='afdID', keep=False)]\n",
    "print(f\"\\t[{len(duplicates)}] remaining duplicate 'afdID'.\\n\")\n",
    "del duplicates\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a0fc4-35bb-45d7-8556-69182afcf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = afds_ll_fires[afds_ll_fires.duplicated(subset='afdID', keep=False)]\n",
    "print(f\"\\t[{len(duplicates)}] remaining duplicate 'afdID'.\\n\")\n",
    "duplicates.to_file(os.path.join(dataAFD,\"duplicateFIRED_afds.gpkg\"))\n",
    "\n",
    "# Explore duplicates further\n",
    "#print(duplicates[['afdID','id','tot_ar_km2','longitude', 'latitude', 'confidence', 'frp', 'acq_date', 'acq_time', 'daynight', 'satellite']].head())\n",
    "\n",
    "# How many unique afdIDs\n",
    "print(duplicates['afdID'].nunique)\n",
    "\n",
    "# Area difference between duplicates\n",
    "duplicates['dupDif'] = duplicates.groupby('afdID')['tot_ar_km2'].transform(lambda x: x.max() - x.min())\n",
    "print(duplicates[['afdID','merge_id','tot_ar_km2','dupDif']].head())\n",
    "\n",
    "# Sort by 'Age' in descending order\n",
    "sorted_dups = duplicates.sort_values(by='dupDif', ascending=False)\n",
    "print(sorted_dups[['afdID','merge_id','tot_ar_km2','dupDif']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT: HERE WE REMOVE DUPLICATES BY ONLY KEEPING THE AFD ASSOCIATED WITH THE LARGER FIRE PERIM\n",
    "print(f\"\\nNumber of rows in afds_ll_fires before 'afdID' deduplication (keeping largest fire): {len(afds_ll_fires)}\")\n",
    "\n",
    "# Sort by afdID and then by tot_ar_km2 (descending) to ensure the largest area comes first for each afdID\n",
    "afds_ll_fires = afds_ll_fires.sort_values(['afdID', 'tot_ar_km2'], ascending=[True, False])\n",
    "\n",
    "# Drop duplicates based on afdID, keeping the first occurrence (which now has the largest tot_ar_km2)\n",
    "afds_ll_fires = afds_ll_fires.drop_duplicates(subset='afdID', keep='first')\n",
    "\n",
    "# Print number of rows after deduplication\n",
    "print(f\"Number of rows in afds_ll_fires after 'afdID' deduplication: {len(afds_ll_fires)}\")\n",
    "\n",
    "# Verify that there are no more duplicates for 'afdID'\n",
    "remaining_duplicates_check = afds_ll_fires[afds_ll_fires.duplicated(subset='afdID', keep=False)]\n",
    "print(f\"Number of 'afdID' groups with multiple entries after removal: {len(remaining_duplicates_check)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8082e42-874e-49e8-aeee-edeba8c0f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the day/night times in MST\n",
    "day_obs = afds_ll_fires[afds_ll_fires['daynight'] == 'D']\n",
    "day_obs['acq_time_mst'] = day_obs['acq_dt_mst'].dt.time\n",
    "print(f\"\\nFirst MST datetime for 'Day': {day_obs['acq_time_mst'].min()}\")\n",
    "print(f\"Last MST datetime for 'Day': {day_obs['acq_time_mst'].max()}\\n\")\n",
    "\n",
    "# check on nighttime datetimes\n",
    "night_obs = afds_ll_fires[afds_ll_fires['daynight'] == 'N']\n",
    "night_obs['acq_time_mst'] = night_obs['acq_dt_mst'].dt.time\n",
    "print(f\"First MST datetime for 'Night': {night_obs['acq_time_mst'].min()}\")\n",
    "print(f\"Last MST datetime for 'Night': {night_obs['acq_time_mst'].max()}\\n\")\n",
    "del night_obs, day_obs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96c5a6",
   "metadata": {},
   "source": [
    "## **Step 6a: Remove scan duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36115ecb-a575-4acf-a4f4-23fcd9a6bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle duplicates caused by the adjacent scan overlap\n",
    "from scipy.spatial import cKDTree # for distance matrix\n",
    "# same datetime, distance between detections smaller than the along_track distance\n",
    "drop_obs = set() # unique detections to drop\n",
    "# Group observations by Fire_ID and acquisition datetime\n",
    "## Changed to identify by FIRED id which is just an integer\n",
    "dt_groups = afds_ll_fires.groupby(['merge_id', 'acq_dt_mst'])\n",
    "print(f\"\\nNumber of unique (FIRED merge_id, acq_datetime) groups: {len(dt_groups)}\\n\")\n",
    "\n",
    "# process the datetime groups\n",
    "def process_afd_duplicates(group):\n",
    "    \"\"\"Identify duplicates in a FIRED merge_id/datetime group using KDTree\"\"\"\n",
    "    if len(group) <= 1:\n",
    "        return  # Skip groups with a single point\n",
    "    # Extract point coordinates and thresholds\n",
    "    coords = np.array([[geom.x, geom.y] for geom in group.geometry])\n",
    "    thresholds = group['track'].values / 2 * 1000  # Convert along-track distance to meters\n",
    "    # Build a KDTree for the group points\n",
    "    tree = cKDTree(coords)\n",
    "\n",
    "    # Query KDTree for neighbors within the maximum threshold distance\n",
    "    for i, (coord, threshold) in enumerate(zip(coords, thresholds)):\n",
    "        if group.iloc[i]['afdID'] in drop_obs:\n",
    "            continue  # Skip already marked duplicates\n",
    "        \n",
    "        # Find neighbors within the threshold distance\n",
    "        neighbor_idxs = tree.query_ball_point(coord, threshold)\n",
    "\n",
    "        # Compare with neighbors and flag duplicates\n",
    "        for j in neighbor_idxs:\n",
    "            if i == j or group.iloc[j]['afdID'] in drop_obs:\n",
    "                continue  # Skip self-comparison and already marked duplicates\n",
    "            \n",
    "            # Compare FRP values to determine which detection to drop\n",
    "            if group.iloc[j]['frp'] < group.iloc[i]['frp']:\n",
    "                drop_obs.add(group.iloc[j]['afdID'])  # Drop the neighbor\n",
    "            else:\n",
    "                drop_obs.add(group.iloc[i]['afdID'])  # Drop the current point\n",
    "                break  # Stop further checks for this point\n",
    "\n",
    "# Process each group efficiently\n",
    "for (fire_id, datetime), group in dt_groups:\n",
    "    process_afd_duplicates(group)\n",
    "print(f\"Identified [{len(drop_obs)} ({round(len(drop_obs)/len(afds_ll_fires)*100,2)}%)] duplicates.\\n\")\n",
    "# Remove duplicates from processing\n",
    "afds_ll_fires = afds_ll_fires[~afds_ll_fires['afdID'].isin(drop_obs)]\n",
    "print(f\"\\t[{len(afds_ll_fires)}] remaining detections.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113980e",
   "metadata": {},
   "source": [
    "## **Step 7: Observation counts and FRP historgram --> need to chunk better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a225da1-763c-45a3-98f8-67257c44fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a count of observations for each fire\n",
    "counts = afds_ll_fires.groupby(['merge_id']).size().reset_index(name='count')\n",
    "\n",
    "print(counts.head())\n",
    "print(counts.columns)\n",
    "\n",
    "afds_ll_fires = pd.merge(afds_ll_fires, counts, on='merge_id', how='left')\n",
    "\n",
    "# print some statistics\n",
    "print(\"\\n\", afds_ll_fires['count'].describe(), \"\\n\")\n",
    "print(f\"10th percentile count: {afds_ll_fires['count'].quantile(0.10)}\", \"\\n\") \n",
    "\n",
    "# plot the distribution\n",
    "plt.figure(figsize=(5, 3))  # Set figure size\n",
    "sns.histplot(counts['count'], bins=30, kde=True, color='skyblue')\n",
    "plt.xlabel('# of detections', fontsize=10)\n",
    "plt.ylabel('frequency', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda8584-5d50-45be-a8ad-4c3985634adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quickly calculate pix_area by multiplying scan and track\n",
    "afds_ll_fires['pix_area'] = afds_ll_fires['scan'] * afds_ll_fires['track']\n",
    "\n",
    "# check on the FRP measurements\n",
    "# identify observations with 0 FRP\n",
    "print(f\"\\n{afds_ll_fires['frp'].describe()}\\n\")\n",
    "# drop 0 records (no FRP measurements)\n",
    "n_zero = afds_ll_fires[afds_ll_fires['frp'] == 0]['frp'].count()\n",
    "afds_ll_fires = afds_ll_fires[afds_ll_fires['frp'] > 0]\n",
    "print(f\"\\tRemoved [{n_zero}] observations with FRP == 0\\n\")\n",
    "\n",
    "# create a \"W/Km2\" column, dividing FRP by pixel area\n",
    "afds_ll_fires['frp_wkm2'] = afds_ll_fires['frp'] / afds_ll_fires['pix_area']\n",
    "print(f\"\\n{afds_ll_fires['frp_wkm2'].describe()}\\n\")\n",
    "print(afds_ll_fires[['frp','frp_wkm2','pix_area']].head(3), \"\\n\")\n",
    "\n",
    "# plot the distribution\n",
    "x_min, x_max = afds_ll_fires['frp'].min(), afds_ll_fires['frp'].max()\n",
    "plt.figure(figsize=(5, 3))  # Set figure size\n",
    "sns.histplot(afds_ll_fires['frp'], bins=50, kde=True, color='red')\n",
    "plt.xlim(x_min, x_max)\n",
    "#plt.ylim(0, 170000)\n",
    "plt.xlabel('fire radiative power', fontsize=10)\n",
    "plt.ylabel('frequency', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08a440-558f-41d4-af28-e222a309b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on some detections thresholds by fire\n",
    "print(f\">=10 obs: {len(afds_ll_fires[afds_ll_fires['count'] >= 10]['merge_id'].unique())}\")\n",
    "print(f\">=50 obs: {len(afds_ll_fires[afds_ll_fires['count'] >= 50]['merge_id'].unique())}\")\n",
    "print(f\">=100 obs: {len(afds_ll_fires[afds_ll_fires['count'] >= 100]['merge_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a0219-86b1-49f7-a002-01383a70b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep fires with enough detections\n",
    "n_obs = 10 # the threshold for number of observations\n",
    "afds_fires = afds_ll_fires[afds_ll_fires['count'] >= n_obs]\n",
    "print(f\"There are {len(afds_fires['merge_id'].unique())} fires with >= {n_obs} obs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ac45e-4416-4247-81d8-d499d70acce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this file out.\n",
    "out_fp = os.path.join(dataAFD,'viirs_snpp_jpss1_afd_latlon_FIRED_WEST_fires.gpkg')\n",
    "afds_fires.to_file(out_fp)\n",
    "print(f\"Saved spatial points to: {out_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f1fe0-bfb6-4e98-8efb-ea2f55e08568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy up\n",
    "del afds_ll_fires, afds_ll\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ac29a",
   "metadata": {},
   "source": [
    "## **Step 9: Convert AFD to pixel area dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb5f44-037e-4433-b44c-906f8e745a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ground area of pixels based on swath position\n",
    "# Define the pixel buffer function for the given width and height\n",
    "def pixel_area(point, width, height):\n",
    "    half_width = width / 2\n",
    "    half_height = height / 2\n",
    "    return box(\n",
    "        point.x - half_width, point.y - half_height,\n",
    "        point.x + half_width, point.y + half_height\n",
    "    )\n",
    "\n",
    "afds_pix = afds_fires.copy() # work on a copy of our lat/lon detections\n",
    "\n",
    "# Apply the buffer function with along_scan and along_track values converted to meters (*1000)\n",
    "afds_pix[\"geometry\"] = afds_pix.apply(\n",
    "    lambda row: pixel_area(row[\"geometry\"], row[\"scan\"] * 1000, row[\"track\"] * 1000), axis=1\n",
    ")\n",
    "\n",
    "# tidy the index column\n",
    "afds_pix = afds_pix.reset_index(drop=True)\n",
    "afds_pix['obs_id'] = afds_pix.index # unique ID column\n",
    "print(f\"Total detections at this stage: {len(afds_pix)}\")\n",
    "print(f\"\\nColumns available:\\n {afds_pix.columns}\\n\")\n",
    "afds_pix[['longitude', 'latitude', 'confidence', 'frp',\n",
    "          'acq_date', 'acq_time', 'acq_dt_mst', 'daynight', 'satellite']].head() # check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7522a5-77cc-408d-9d54-bf51660d36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this file out.\n",
    "out_fp = os.path.join(dataAFD,'viirs_snpp_jpss1_afd_latlon_firesFIRED_west_pixar.gpkg')\n",
    "afds_pix.to_file(out_fp)\n",
    "print(f\"Saved to {out_fp}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5e741-e711-492a-93ab-99a8eadf767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset fires to the grid dataset\n",
    "fires = fires[fires['merge_id'].isin(afds_pix['merge_id'].unique())]\n",
    "# export the new fire census\n",
    "print(f\"Exporting {len(fires)}\")\n",
    "out_fp = os.path.join(dataFires,'aoi_fireFIRED_census_2000_to_2024_subset.gpkg')\n",
    "fires.to_file(out_fp)\n",
    "print(f\"Saved file to: {out_fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793b33c",
   "metadata": {},
   "source": [
    "## **Step 10: Aggregate FRP to a regular grid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844b67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec1fe2-21b3-4a45-9928-3d4d047cac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate FRP to a regular grid\n",
    "t0 = time.time()\n",
    "\n",
    "# create a regular grid extracted to fire perimeters\n",
    "def regular_grid(extent, res=0.0035, crs_out='EPSG:5070', regions=None):\n",
    "    \"\"\"\n",
    "    Creates a regular-spaced grid\n",
    "    \"\"\"\n",
    "    # retrieve bounding coordinates\n",
    "    min_lon, max_lon, min_lat, max_lat = extent\n",
    "    \n",
    "    # create the grid lines in degrees\n",
    "    x_coords = np.arange(min_lon, max_lon, res)\n",
    "    y_coords = np.arange(min_lat, max_lat, res)\n",
    "\n",
    "    # generate the grid cells\n",
    "    cells = [\n",
    "        Polygon([(x, y), (x + res, y), (x + res, y + res), (x, y + res)])\n",
    "        for x in x_coords for y in y_coords\n",
    "    ]\n",
    "\n",
    "    # create a geodataframe in WGS, reprojected if needed\n",
    "    grid = gpd.GeoDataFrame({'geometry': cells}, crs=crs_out)\n",
    "\n",
    "    if regions is not None:\n",
    "        if regions.crs != grid.crs:\n",
    "            regions = regions.to_crs(grid.crs)\n",
    "        # Perform spatial intersection to keep only grid cells overlapping the polygon\n",
    "        grid = grid[grid.intersects(regions.unary_union)].copy()\n",
    "\n",
    "    return grid\n",
    "\n",
    "# get the AOI extent in lat/lon (WGS)\n",
    "coords, extent = get_coords(aoi, buffer=1000, crs='EPSG:5070')\n",
    "print(f\"Bounding extent for the AOI: {extent}\")\n",
    "\n",
    "# --- Add this block to select 5 random fires for testing ---\n",
    "num_random_fires_to_select = 5\n",
    "\n",
    "if not fires.empty and len(fires) >= num_random_fires_to_select:\n",
    "    fires_for_testing = fires.sample(n=num_random_fires_to_select, random_state=1).copy()\n",
    "    # random_state=1 is used for reproducibility; change or remove if you want different random samples each time.\n",
    "    print(f\"Randomly selected {len(fires_for_testing)} fires for testing.\")\n",
    "elif not fires.empty: # If there are fires, but fewer than 5\n",
    "    fires_for_testing = fires.copy()\n",
    "    print(f\"Warning: Fewer than {num_random_fires_to_select} fires available. Using all {len(fires_for_testing)} available fires for testing.\")\n",
    "else: # If 'fires' is empty\n",
    "    print(\"Warning: The 'fires' GeoDataFrame is empty. No fires to select for testing. 'regions' for gridding will be effectively empty.\")\n",
    "    # Create an empty GeoDataFrame with the same schema if 'fires' itself is empty, to avoid downstream errors\n",
    "    # expecting 'fires_for_testing' to exist.\n",
    "    fires_for_testing = fires.copy() # This will be an empty GeoDataFrame\n",
    "# --- End of random sampling block ---\n",
    "\n",
    "\n",
    "# generate the grid (0.0035 degrees or 375m)\n",
    "# extract grid intersecting fire perimeters\n",
    "grid = regular_grid(\n",
    "    extent, # AOI\n",
    "    res=375, # 375m2 (VIIRS at-nadir res)\n",
    "    crs_out='EPSG:5070', # projected crs\n",
    "    regions=fires # any overlapping detections\n",
    ")\n",
    "\n",
    "# save this out.\n",
    "out_fp = os.path.join(dataFires, 'aoi_fireFIRED_west_census_reggrid_375m.gpkg')\n",
    "grid.to_file(out_fp, driver=\"GPKG\")\n",
    "print(f\"Grid saved to: {out_fp}\")\n",
    "\n",
    "t1 = (time.time() - t0) / 60\n",
    "print(f\"\\nTotal elapsed time: {t1:.2f} minutes.\\n\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0722a2b",
   "metadata": {},
   "source": [
    "## **Step 11: Calculate cumulative FRP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0faeab-f539-4a47-ac93-3933649cc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a gridded cumulative FRP and maximum FRP\n",
    "# process the fire data by aggregating fractional FRP\n",
    "t0 = time.time()\n",
    "\n",
    "def aggregate_frp(detections, grid):\n",
    "    \"\"\"\n",
    "    Aggregate fire pixel frp data into a regular grid with fractional overlay\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the projections match\n",
    "    if detections.crs != grid.crs:\n",
    "        detections = detections.to_crs(grid.crs)\n",
    "\n",
    "    # calculate the pixel area in m2\n",
    "    detections['pix_area_m2'] = detections['pix_area'] * 1e6\n",
    "\n",
    "    # tidy the grid and calculate the grid area\n",
    "    grid = grid.reset_index(drop=False).rename(columns={'index': 'grid_index'})\n",
    "    grid['grid_area'] = grid.geometry.area  # precompute grid cell areas\n",
    "\n",
    "    # overlay detections onto the grid\n",
    "    overlay = gpd.overlay(detections, grid, how='intersection')\n",
    "\n",
    "    # fractional overlap area\n",
    "    overlay['overlap_m2'] = overlay.geometry.area\n",
    "    overlay['fraction'] = overlay['overlap_m2'] / overlay['pix_area_m2']\n",
    "    \n",
    "    # multiply FRP by fractional area\n",
    "    overlay['frp_fr'] = overlay['frp_wkm2'] * overlay['fraction']\n",
    "    overlay['frp_fr'] = overlay['frp_fr'].fillna(0)\n",
    "    \n",
    "    # Aggregate by grid cell\n",
    "    aggregated = overlay.groupby('grid_index').agg(\n",
    "        afd_count=('frp_fr', 'count'), \n",
    "        unique_days=('acq_date', 'nunique'),\n",
    "        # overlap amount\n",
    "        overlap=('fraction', 'sum'),\n",
    "        # fire radiative power\n",
    "        frp_csum=('frp_fr', 'sum'),\n",
    "        frp_max=('frp_fr', 'max'),\n",
    "        frp_min=('frp_fr', 'min'),\n",
    "        frp_mean=('frp_fr', 'mean'),\n",
    "        frp_p90=('frp_fr', lambda x: x.quantile(0.90) if not x.empty else 0),\n",
    "        frp_p95=('frp_fr', lambda x: x.quantile(0.95) if not x.empty else 0),\n",
    "        frp_p97=('frp_fr', lambda x: x.quantile(0.97) if not x.empty else 0),\n",
    "        frp_p99=('frp_fr', lambda x: x.quantile(0.99) if not x.empty else 0),\n",
    "        frp_first=('frp_fr', lambda x: overlay.loc[x.index, :].sort_values('acq_dt_mst').iloc[0]['frp_fr']),\n",
    "        day_max_frp=('frp_fr', lambda x: overlay.loc[x.idxmax(), 'acq_date'] if not x.empty else None),\n",
    "        dt_max_frp=('frp_fr', lambda x: overlay.loc[x.idxmax(), 'acq_dt_mst'] if not x.empty else None),\n",
    "        first_obs_date=('acq_date', 'min'),\n",
    "        last_obs_date=('acq_date', 'max') \n",
    "    ).reset_index()\n",
    "\n",
    "    # add day and night FRP stats\n",
    "    daynight_stats = (\n",
    "        overlay.groupby(['grid_index', 'daynight'])['frp_fr']\n",
    "        .agg(\n",
    "            count='count',\n",
    "            max='max', \n",
    "            mean='mean',\n",
    "            sum='sum',\n",
    "            p90=lambda x: x.quantile(0.90) if not x.empty else 0,\n",
    "            p95=lambda x: x.quantile(0.95) if not x.empty else 0,\n",
    "            p97=lambda x: x.quantile(0.97) if not x.empty else 0,\n",
    "            p99=lambda x: x.quantile(0.99) if not x.empty else 0,\n",
    "            first=lambda x: overlay.loc[x.index, :].sort_values('acq_dt_mst').iloc[0]['frp_fr'],\n",
    "            last=lambda x: overlay.loc[x.index, :].sort_values('acq_dt_mst', ascending=False).iloc[0]['frp_fr']\n",
    "        )\n",
    "        .unstack(fill_value=0)  # Ensure missing values are filled with 0\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    #print(daynight_stats.columns)\n",
    "\n",
    "    # Flatten column names after unstacking\n",
    "    daynight_stats.columns = [\n",
    "        f\"{stat}_{dn}\" if dn else stat\n",
    "        for stat, dn in daynight_stats.columns\n",
    "    ]\n",
    "\n",
    "    # Rename flattened daynight_stats columns to match original naming convention\n",
    "    rename_map = {\n",
    "        'count_D': 'day_count',\n",
    "        'count_N': 'night_count',\n",
    "        'max_D': 'frp_max_day',\n",
    "        'max_N': 'frp_max_night',\n",
    "        'sum_D': 'frp_csum_day',\n",
    "        'sum_N': 'frp_csum_night',\n",
    "        'mean_D': 'frp_mean_day',\n",
    "        'mean_N': 'frp_mean_night',\n",
    "        'p90_D': 'frp_p90_day',\n",
    "        'p90_N': 'frp_p90_night',\n",
    "        'p95_D': 'frp_p95_day',\n",
    "        'p95_N': 'frp_p95_night',\n",
    "        'p97_D': 'frp_p97_day',\n",
    "        'p97_N': 'frp_p97_night',\n",
    "        'p99_D': 'frp_p99_day',\n",
    "        'p99_N': 'frp_p99_night',\n",
    "        'first_D': 'frp_first_day',\n",
    "        'first_N': 'frp_first_night',\n",
    "        'last_D': 'frp_last_day',\n",
    "        'last_N': 'frp_last_night'\n",
    "    }\n",
    "    \n",
    "    daynight_stats = daynight_stats.rename(columns=rename_map)\n",
    "\n",
    "    # Merge the flattened daynight_stats back in on 'grid_index'\n",
    "    aggregated = aggregated.merge(daynight_stats, on='grid_index', how='left')\n",
    "\n",
    "    # Fill missing values (if any grid cell lacks day/night data)\n",
    "    aggregated = aggregated.fillna(0)\n",
    "    \n",
    "    # Add day/night statistics\n",
    "    #aggregated['day_count'] = daynight_stats['count'].get('D', 0)\n",
    "    #aggregated['night_count'] = daynight_stats['count'].get('N', 0)\n",
    "    #aggregated['frp_max_day'] = daynight_stats['max'].get('D', 0)\n",
    "    #aggregated['frp_max_night'] = daynight_stats['max'].get('N', 0)\n",
    "    #aggregated['frp_csum_day'] = daynight_stats['sum'].get('D', 0)\n",
    "    #aggregated['frp_csum_night'] = daynight_stats['sum'].get('N', 0)\n",
    "    #aggregated['frp_mean_day'] = daynight_stats['mean'].get('D', 0)\n",
    "    #aggregated['frp_mean_night'] = daynight_stats['mean'].get('N', 0)\n",
    "    #aggregated['frp_p90_day'] = daynight_stats['p90'].get('D', 0)\n",
    "    #aggregated['frp_p90_night'] = daynight_stats['p90'].get('N', 0)\n",
    "    #aggregated['frp_p95_day'] = daynight_stats['p95'].get('D', 0)\n",
    "    #aggregated['frp_p95_night'] = daynight_stats['p95'].get('N', 0)\n",
    "    #aggregated['frp_p97_day'] = daynight_stats['p97'].get('D', 0)\n",
    "    #aggregated['frp_p97_night'] = daynight_stats['p97'].get('N', 0)\n",
    "    #aggregated['frp_p99_day'] = daynight_stats['p99'].get('D', 0)\n",
    "    #aggregated['frp_p99_night'] = daynight_stats['p99'].get('N', 0)\n",
    "    #aggregated['frp_first_day'] = daynight_stats['first'].get('D', 0)\n",
    "    #aggregated['frp_first_night'] = daynight_stats['first'].get('N', 0)\n",
    "    \n",
    "    # Join results back to grid\n",
    "    grid = grid.merge(aggregated, on='grid_index', how='right')\n",
    "        \n",
    "    return grid\n",
    "\n",
    "\n",
    "# Make sure we are using fires with >=50 detections\n",
    "fires_ = fires[fires['merge_id'].isin(afds_pix['merge_id'].unique())]\n",
    "fire_grids = [] # Initialize results list\n",
    "with tqdm(fires_.iterrows(), total=len(fires_)) as pbar:\n",
    "    for _, fire in pbar:\n",
    "        pbar.set_description(f\"Processing [{fire['merge_id']}]\")\n",
    "        # get the geodataframe of the fire\n",
    "        fire_gdf = gpd.GeoDataFrame([fire], crs=fires_.crs)  # Ensure GeoDataFrame\n",
    "        \n",
    "        # aggregate fire pixels to the grid\n",
    "        fire_grid = aggregate_frp(afds_pix[afds_pix['merge_id'] == fire['merge_id']], grid)\n",
    "        fire_grid['merge_id'] = fire['merge_id']\n",
    "        #fire_grid['Fire_Name'] = fire['Fire_Name']\n",
    "        fire_grids.append(fire_grid)\n",
    "\n",
    "# Combine all grids into one\n",
    "fire_grids = pd.concat(fire_grids)\n",
    "\n",
    "t3 = (time.time() - t0) / 60\n",
    "print(f\"\\nTotal elapsed time: {t3:.2f} minutes.\\n\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113c774-dd4e-4884-adf5-e97bcf20b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fire_grids['afd_count'].describe())\n",
    "print(\"/n\")\n",
    "print(fire_grids['unique_days'].describe())\n",
    "list(fire_grids.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a787a8-4df9-4605-9e74-8a1fd63b1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to change a few of the column names here because of using a flattened df instead of a multiindex\n",
    "fire_grids.sort_values(\n",
    "    by='frp_csum', ascending=False\n",
    ")[['grid_index','frp_csum','frp_csum_day','frp_max','frp_max_day',\n",
    "   'frp_p90', 'frp_p95', 'frp_p97', 'frp_p99','frp_min','frp_first',\n",
    "   'frp_first_day', 'day_count','night_count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89505499-17b7-417b-a1c1-d032c97cfe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cumulative FRP (total): {round(fire_grids['frp_csum'].sum(), 2)} (W/km2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf0efa-590c-4b09-824f-1e489991e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this file out.\n",
    "out_fp = os.path.join(dataAFD,'viirs_snpp_jpss1_afd_latlon_firesFIRED_WEST_pixar_gridstats.gpkg')\n",
    "fire_grids.to_file(out_fp)\n",
    "print(f\"Saved file to: {out_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ceab4-82ff-4aa8-a195-904b78411756",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_grids.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f922a1-4dc6-4afd-a70d-ab43323c4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71214369-456c-4a7f-af95-458ea3aa8e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c3a41-cb73-410e-aa5a-e99b7f04f9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28ad6923-33ba-4c48-8316-1807d966fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Earth Engine exports ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5c97f-7b03-4f53-abd9-71b737589fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pixel ground area data\n",
    "afds_pix_gee = afds_pix.copy()\n",
    "\n",
    "# tidy the date columns\n",
    "afds_pix_gee.rename(columns={\n",
    "    'ig_date': 'Ig_Date', \n",
    "    'last_date': 'Last_Date'\n",
    "}, inplace=True)\n",
    "\n",
    "# convert to string for GEE\n",
    "date_cols = ['acq_date', 'Ig_Date', 'Last_Date']\n",
    "for col in date_cols:\n",
    "    afds_pix_gee[col] = afds_pix_gee[col].dt.date.astype(str)\n",
    "\n",
    "# subset columns\n",
    "afds_pix_gee = afds_pix_gee[['id','afdID','acq_date','daynight',\n",
    "                             'Ig_Date','Last_Date','geometry']]\n",
    "print(afds_pix_gee.dtypes) # make sure the formats are correct\n",
    "\n",
    "# export shapefile\n",
    "out_fp = os.path.join(projdir, 'data/earth-engine/imports/viirs_snpp_jpss1_afd_latlon_fires_pixar.zip')\n",
    "tempdir = os.path.join(projdir, 'data/tempdir')\n",
    "save_zip(afds_pix_gee, out_fp, tempdir) # see __functions.py\n",
    "print(f\"Saved to:{out_fp}\")\n",
    "\n",
    "del afds_pix_gee\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761cb21-bfb8-47cc-a05c-fa83194ba5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save a fire perimeter dataset for GEE\n",
    "# subset the fire perimeter data\n",
    "# fires with >= 50 detections\n",
    "fires_gee = fires.copy() # work on a copy\n",
    "fires_gee = fires_gee[fires_gee['id'].isin(afds_pix['id'].unique())]\n",
    "print(f\"There are {len(fires_gee)} with >= {n_obs} detections.\")\n",
    "\n",
    "# subset columns\n",
    "fires_gee = fires_gee[['id','ig_year',\n",
    "                       'ig_date','WF_CESSATION_DATE','geometry']]\n",
    "fires_gee.rename(columns={\n",
    "    'START_YEAR': 'Fire_Year', \n",
    "    'DISCOVERY_DATE': 'Ig_Date', \n",
    "    'WF_CESSATION_DATE': 'Last_Date'\n",
    "}, inplace=True)\n",
    "# tidy the date columns\n",
    "fires_gee['Ig_Date'] = fires_gee['Ig_Date'].dt.date.astype(str)\n",
    "fires_gee['Last_Date'] = fires_gee['Last_Date'].dt.date.astype(str)\n",
    "\n",
    "# export the file to a zip archive\n",
    "out_fp = os.path.join(projdir, 'data/earth-engine/imports/srm_fire_census_w_afds_2017to2023.zip')\n",
    "save_zip(fires_gee, out_fp, tempdir) # see __functions.py\n",
    "print(f\"\\nSaved to:{out_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30966d-ca06-4ef5-8e56-56c7cacf6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the gridstats data for GEE\n",
    "fire_grids_gee = fire_grids.copy()\n",
    "\n",
    "# tidy the columns\n",
    "fire_grids_gee = fire_grids_gee[['grid_index','Fire_ID','afd_count',\n",
    "                                 'day_max_frp','first_obs_date','last_obs_date',\n",
    "                                 'geometry']]\n",
    "fire_grids_gee.rename(columns={\n",
    "    'day_max_frp': 'max_date', \n",
    "    'first_obs_date': 'first_obs', \n",
    "    'last_obs_date': 'last_obs'}, inplace=True)\n",
    "    \n",
    "# join in the fire ignition dates\n",
    "fire_grids_gee = fire_grids_gee.merge(fires_gee[['Fire_ID', 'Fire_Year', 'Ig_Date', 'Last_Date']], on='Fire_ID', how='left')\n",
    "\n",
    "# handle date fields for GEE\n",
    "date_cols = ['max_date', 'first_obs', 'last_obs', 'Ig_Date', 'Last_Date']\n",
    "for col in date_cols:\n",
    "    fire_grids_gee[col] = fire_grids_gee[col].astype(str)\n",
    "\n",
    "out_fp = os.path.join(projdir,'data/earth-engine/imports/viirs_snpp_jpss1_afd_gridstats.zip')\n",
    "save_zip(fire_grids_gee, out_fp, tempdir) # see __functions.py\n",
    "print(f\"Exported layer to: {out_fp}\")\n",
    "\n",
    "print(f\"\\n{fire_grids_gee.head()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb11dc-0a55-4168-ae67-049f1f6d14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dissolved grid by day of first detection for GridMET analysis\n",
    "# Dissolve by first_obs\n",
    "grid_gee_dis = fire_grids_gee.dissolve(by=['Fire_ID','first_obs'])\n",
    "# Reset the index to make `first_obs` a regular column\n",
    "grid_gee_dis = grid_gee_dis.reset_index()\n",
    "print(grid_gee_dis.columns)\n",
    "\n",
    "# Save this file out.\n",
    "out_fp = os.path.join(projdir,'data/earth-engine/imports/viirs_snpp_jpss1_afd_gridstats_days.zip')\n",
    "save_zip(grid_gee_dis, out_fp, tempdir) # see __functions.py\n",
    "print(f\"Exported layer to: {out_fp}\")\n",
    "\n",
    "del fire_grids_gee, fires_gee, grid_gee_dis\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec28c10-c8ab-49f9-915f-8647b04560f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2a458-57b8-41c8-b89b-f325d5a31210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fire bounds for GEE CBI export\n",
    "bounds = fires.copy()\n",
    "# Create the bounds with a generous buffer\n",
    "bounds['geometry'] = bounds.geometry.envelope.buffer(3000)\n",
    "\n",
    "# Load state boundaries\n",
    "states = os.path.join(maindir, 'data/boundaries/political/TIGER/tl19_us_states_w_ak_lambert.gpkg')\n",
    "states = gpd.read_file(states)\n",
    "states = states.to_crs('EPSG:5070')\n",
    "\n",
    "# Grab the fire centroid\n",
    "centroid = bounds.copy()\n",
    "centroid['geometry'] = centroid.geometry.centroid\n",
    "fire_state = gpd.overlay(centroid[['Fire_ID','geometry']], states[['STUSPS','geometry']], how='intersection')\n",
    "fire_state = fire_state[['Fire_ID','STUSPS']]\n",
    "bounds = pd.merge(bounds, fire_state, on='Fire_ID', how='left')\n",
    "\n",
    "# Assign the Parks et al. (2019) start and end days\n",
    "special_case = ['Arizona', 'New Mexico']\n",
    "bounds['Start_Day'] = None\n",
    "bounds['End_Day'] = None    \n",
    "bounds.loc[bounds['STUSPS'].isin(special_case), ['Start_Day', 'End_Day']] = (91, 181)\n",
    "bounds.loc[~bounds['STUSPS'].isin(special_case), ['Start_Day', 'End_Day']] = (152, 258)\n",
    "# Tidy the columns for exporting\n",
    "bounds = bounds[['Fire_ID','Fire_Name','DISCOVERY_DATE','WF_CESSATION_DATE','START_YEAR','Start_Day','End_Day','geometry']]\n",
    "bounds.rename(columns={\n",
    "    'START_YEAR': 'Fire_Year',\n",
    "    'DISCOVERY_DATE': 'Ig_Date',\n",
    "    'WF_CESSATION_DATE': 'Last_Date'\n",
    "}, inplace=True)\n",
    "bounds['Ig_Date'] = bounds['Ig_Date'].dt.date.astype(str)\n",
    "bounds['Last_Date'] = bounds['Last_Date'].dt.date.astype(str)\n",
    "print(bounds.head())\n",
    "\n",
    "# Export to a zipfile\n",
    "out_fp = os.path.join(projdir,'data/earth-engine/imports/srm_fire_census_2017_to_2023_bounds.zip')\n",
    "save_zip(bounds, out_fp, tempdir) # see __functions.py\n",
    "print(f\"Exported layer to: {out_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bc625-d0df-4969-9e68-b206021a1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
